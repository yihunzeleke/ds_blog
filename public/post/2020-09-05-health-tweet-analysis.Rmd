---
title: Health Tweet analysis
author: Yihun Zeleke
date: '2020-09-05'
categories:
  - R
tags:
  - data wragling
  - sentiment-analysis
  - regression
  - text mining
  - topic modeling
  - visualization
slug: health-tweet-analysis
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE)
```
## Project idea
There are different health tweets from different news agencies. This health tweets from bbc, cbc,and many more.
The data is publicly available [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00438/).


![](/post/2020-09-05-health-tweet-analysis_files/socialmediatwitter.jpg)
<br/>

## Data pre-processing

I will start by reading the all the `16` health tweets from news agencies.

```{r}
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(ggplot2)
library(textdata)
theme_set(theme_bw())


```

```{r}
raw_text <- read_csv("https://raw.githubusercontent.com/yihunzeleke/data_repo/master/raw_text.csv")



raw_text

```

The `newsgroup` column which describes which of the 16 newsgroups each tweets comes from. And the `id` column which generated from Tweeter while `date` is the date of tweets that made by the news agencies and the `text` column is the tweets from the news agencies.   

What news Agencies are included, and how many tweets were posted in each?

```{r}

library(ggplot2)

raw_text %>% 
  group_by(newsgroup) %>%
  count(newsgroup, sort = T) %>% 
  rename(`# tweets` = n) %>% 
   ggplot(aes(newsgroup,`# tweets`))+
  geom_col()+
  coord_flip()
  

```


As we from newsgroups `goodhealth` and `nytimeshealth` have  higher tweets than others. 

### Pre-processing text
Text pre-processing is more complicated than numerical pre-processing. So we use different approach for the text pre-processing to ready for further analysis.  

```{r}
library(stringr)
library(tidytext)
library(SnowballC)

reg_remove <- "&amp;|&lt;|&gt;"

cleaned_text <- raw_text %>% 
  filter(!str_detect(text,"^RT")) %>%  # RT means Retweet
  mutate(text = str_remove_all(text,reg_remove)) %>% 
  #mutate(text =  str_replace_all(text,"#[a-z,A-Z]*","")) %>%  # removing hashtags
  #mutate(text =  str_replace_all(text,"@[a-z,A-Z]*","")) %>% # removing @
  mutate(text = gsub("\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)", "", text)) # Remove https..

cleaned_text  
 
  health_tweets <- cleaned_text %>% 
  unnest_tokens(word,text, token = "tweets") %>% 
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) 
health_tweets

```

### Words in newsgroups  
Let's explore common words in the entire dataset.
```{r}
health_tweets %>% 
    count(word, sort = TRUE)
```

Most common words by newsgroup.

```{r}
words_by_newsgroup <- health_tweets %>% 
  count(newsgroup, word, sort = TRUE)

words_by_newsgroup

```


#### Finding tf-idf within newsgroup  
We see that the newsgroups to differ in terms of topic and content, and therefore the frequency of words to differ between them.

```{r}
tf_idf <-  words_by_newsgroup %>% 
  bind_tf_idf(word, newsgroup,n) %>% 
  arrange(desc(tf_idf))
tf_idf

```

`gdnhealthcare` newsgroups has highest tf_idf, so let's examine to extract words specific to those topics.

```{r}

tf_idf %>% 
  filter(newsgroup %in% c("gdnhealthcare","bbchealth","KaiserHealthNews","wsjhealth","reuters_heatlth")) %>% 
  group_by(newsgroup) %>% 
  top_n(10, tf_idf) %>% 
  ungroup() %>% 
  mutate(word = reorder( word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf, fill = newsgroup))+
  geom_col(show.legend = FALSE)+
  facet_wrap( ~ newsgroup, scales = "free")+
  coord_flip()
```

What newsgroups tend to be similar each other in text content? I could discover this by finding the pairwise correlation of word frequencies within each newsgroup, using `pairwise_cor()` function.

```{r}
library(widyr)

newsgroup_cor <- words_by_newsgroup %>%
  pairwise_cor(newsgroup,  word, n, sort = TRUE)

newsgroup_cor

```

Let's filter stronger correlations among newsgroups, and visualize them in network diagram.

```{r}
library(ggraph)
library(igraph)
set.seed(2015)

newsgroup_cor %>% 
  filter(correlation > 0.4) %>% 
  graph_from_data_frame() %>% 
  ggraph(layout = "fr") +
  geom_edge_link(aes(alpha = correlation, width = correlation))+
  geom_node_point(size = 4, color = "skyblue")+
  geom_node_text(aes(label = name), repel = TRUE)+
  theme_void()

```


Since the tweets from the same topic the newsgroups are inter-correlated, even though as we see from the graph it has mainly three clusters.

## Topic Modeling   
The Latent Dirichlet Allocation(LDA) algorithm to divide a set of texts from from newsgroups that has high tf_idf frequency that we saw earlier.

```{r}
words_newsgroup <- health_tweets %>% 
  filter(newsgroup %in% c("gdnhealthcare","bbchealth","KaiserHealthNews","wsjhealth")) %>% 
  group_by(word) %>% 
  mutate(word_total = n()) %>% 
  ungroup() %>% 
  filter(word_total > 50) %>% 
  mutate(id =  as.character(id), word_total = as.character(word_total))
  
str(words_newsgroup)
```

Convert into a document-term matrix
```{r}
newsgroup_dtm <- words_newsgroup %>% 
  
  unite(document,newsgroup,id) %>% 
  count(document, word) %>% 
  cast_dtm(document,word,n)
  
```

```{r}

library(topicmodels)
newsgroup_lda <- LDA(newsgroup_dtm, k = 4, control = list(seed = 2020))

```

What four topics did this model extract, and did they match the four newsgroups?

```{r}
newsgroup_lda %>% 
  tidy() %>% 
  group_by(topic) %>% 
  top_n(8, beta) %>%
  ungroup() %>% 
  mutate(term = reorder_within(term,beta,topic)) %>% 
  ggplot(aes(term, beta , fill = factor(topic)))+
  geom_col(show.legend = FALSE)+
  facet_wrap( ~  topic, scales = "free_y")+
  coord_flip()+
  scale_x_reordered()
  
```


## Sentiment Analysis

Let's examine how often positive and negative words occurred in the Health tweet posts. Which newsgroup were the most positive or negative word overall?  

I will use the `afinn` sentiment lexicon, which provides numeric positivity values for each word, and visualize with a bar plot.


```{r}
newsgroup_sentiment <- words_by_newsgroup %>% 
  inner_join(get_sentiments("afinn"), by = "word") %>% 
  group_by(newsgroup) %>% 
  summarise(value = sum(value*n) / sum(n))

newsgroup_sentiment

newsgroup_sentiment %>% 
  mutate(newsgroup = reorder(newsgroup, value)) %>% 
  ggplot(aes(newsgroup, value, fill = value > 0))+
  geom_col(show.legend = FALSE)+
  coord_flip()+
  ylab("Average sentiment value")

```


Based on this analysis, the "GoodHealth" newsgroup was the most positive. This makes sense, since it likely included many positive adjectives about health news.

### Sentiment analysis by word  
It's worth looking deeper to understand *why* some newsgroups ended up more positive or negative than others. For that, I can examine the total positive and negative contributions of each word.

```{r}
word_contribution <- health_tweets %>% 
  inner_join(get_sentiments("afinn"), by = "word") %>% 
  group_by(word) %>% 
  summarise(occurrences = n(),
            contribution = sum(value))

word_contribution


word_contribution %>% 
  top_n(25, abs(contribution)) %>% 
  mutate(word = reorder(word , contribution)) %>% 
  ggplot(aes(word, contribution, fill = contribution > 0))+
  geom_col(show.legend = FALSE)+
  coord_flip()


```


These words seems reasonable as indicators of each tweets sentiment, but we can spot possible problems with the approach. "Care" could just as easily be a part of "don't care" or a similar negative expression, and the word "Healthy" are apparently very common on the Health-Tweets but could easily be used in any contexts, positive or negative.  

We may also care about which words contributed most with in each newsgroup, so we can see which newsgroup might be incorrectly estimated. We can calculate each words' contribution to each newsgroup's sentiment score.  


```{r}
top_sentiment_words <- words_by_newsgroup %>% 
  inner_join(get_sentiments("afinn"),by = "word") %>% 
  mutate(contribution = value * n / sum(n))

top_sentiment_words %>% 
  filter(newsgroup == "msnhealthnews", contribution < 0) %>% 
  top_n(10, contribution) %>% 
  arrange(desc(contribution))

top_sentiment_words %>% 
  filter( newsgroup %in% c("goodhealth","gdnhealthcare","msnhealthnews","cbchealth")) %>% 
  group_by(newsgroup) %>%
  top_n(10, abs(contribution)) %>% 
  ungroup() %>% 
  mutate(newsgroup = reorder(newsgroup, contribution),
         word = reorder(paste(word, newsgroup, sep = "_"), contribution)) %>% 
  ggplot(aes(word, contribution, fill = contribution > 0 ))+
  geom_col(show.legend = FALSE)+
  scale_x_discrete(labels = function(x) gsub("_.+","",x))+
  facet_wrap( ~ newsgroup, scales = "free")+
  labs(x = "",
       y = "Sentiment value * # of occurences")+
  coord_flip()

```


This result support our hypothesis about the "GoodHealth" newsgroup: must of the sentiments was driven by positive adjectives such as "healthy","love","happy"... etc.

As we can see here, the sentiment analysis was confounded by topic, for example the word "care","healthy" ,"love" discussed in different newsgroups as positive and "risk","loss" are discussed as negative which may require further examine the influential words before interpreting it.  

### N-gram analysis

Let's take a look the effect of words such as "not" and "no" on the sentiment analysis.

```{r}
tweet_bigram <- cleaned_text %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

tweet_bigram_counts <- tweet_bigram %>% 
  count(newsgroup, bigram, sort = TRUE) %>% 
  separate(bigram, c("word1", "word2"), sep = " ")
```

We could then define a list of six words that we suspect are used in negation, such as "no","not" and "without" and visualize the sentiment-associate words that most often followed them. This shows the words that most often contributed in the "wrong" direction.

```{r}
negate_words <- c("not","without","no","can't", "don't", "won't")

tweet_bigram_counts %>% 
  filter(word1 %in% negate_words) %>% 
  count(word1, word2, wt = n, sort = TRUE) %>% 
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>% 
  mutate(contribution = n*value) %>% 
  group_by(word1) %>% 
  top_n(10, abs(contribution)) %>% 
  ungroup() %>% 
  mutate(word2 = reorder(paste(word2, word1, sep = "_"), contribution)) %>% 
  ggplot(aes(word2, contribution, fill = contribution > 0))+
  geom_col(show.legend = FALSE)+
  facet_wrap( ~ word1, scales = "free", nrow = 3)+
  scale_x_discrete(labels =function(x)gsub("_.+$","",x))+
  xlab("Words preceded by a negation")+
  ylab("Sentiment value * # of occurences")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  coord_flip()
  
```



It looks like the largest sources of misidentifying a word as positive come from “not help/good/perfect”, and the largest source of incorrectly classified negative sentiment is “don't miss”.


## Comparing word usage

Since we have 16 newsgroups, I have select two news agencies randomly *Reuters* and New-*York* health tweets.

```{r}

word_ratios <- health_tweets %>% 
  filter(newsgroup %in% c("reuters_heatlth","nytimeshealth")) %>% 
  count(word, newsgroup) %>% 
  group_by(word) %>% 
  filter(sum(n) >= 25) %>% 
  ungroup() %>% 
  spread(newsgroup, n, fill = 0) %>% 
  mutate_if(is.numeric,list(~(.+1) / (sum(.)+1))) %>% 
  mutate(logratio = log(nytimeshealth / reuters_heatlth)) %>% 
  arrange(desc(logratio))

word_ratios %>%
  group_by(logratio < 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  ylab("log odds ratio (New York Times /Reuters )") +
  scale_fill_discrete(name = "", labels = c("New York Times", "Reuters"))
  
```

So New York times tweets about "Blog","Briefing" on the health tweets, while Reuters tweets health tweeted about "Uk", "astrazeneca", "pharma".

## Changes in word use  
Which words' frequencies have changed the fastest in our Twitter feeds? Or Which words have tweeted about at a higher or lower rate as time has passed?   

```{r}
library(lubridate)

words_by_time <- health_tweets %>% 
  filter(!str_detect(word,"^@"),
         newsgroup %in% c("reuters_heatlth","nytimeshealth")) %>% 
  #filter(newsgroup %in% c("reuters_heatlth","nytimeshealth")) %>%
  mutate(time_floor = floor_date(date, unit = "1 month")) %>% 
  count(time_floor, newsgroup, word) %>% 
  group_by(newsgroup, time_floor) %>% 
  mutate(time_total = sum(n)) %>% 
  group_by(newsgroup, word) %>% 
  mutate(word_total = sum(n)) %>% 
  ungroup() %>% 
  rename(count = n) %>% 
  filter(word_total >= 35)
 
  words_by_time
```

Each row in this data frame corresponds to one newsgroup using one word in a given time bin. The `count` column tells us how many times that person used that word in that time bin, the `time_total` column tells us how many words that person used during that time bin, and the `word_total` column tells us how many times that person used that word over the whole year. This is the data set we can use for modeling.


```{r}
nested_data <- words_by_time %>% 
  nest(-word, -newsgroup)

nested_data

```


> We can think about this modeling procedure answering a question like, “Was a given word mentioned in a given time bin? Yes or no? How does the count of word mentions depend on time?”

```{r}
library(purrr)

nested_model <- nested_data %>% 
  mutate(models = map(data, ~ glm(cbind(count,time_total) ~ time_floor, .,
                      family = "binomial")))
nested_model

```


```{r}
library(broom)

slopes <- nested_model %>% 
  mutate(models = map(models, tidy)) %>% 
  unnest(cols = c(models)) %>% 
  filter(term == "time_floor") %>% 
  mutate(adjusted.p.value = p.adjust(p.value))


top_slopes <- slopes %>% 
  filter(adjusted.p.value < 0.05)

top_slopes


```


Let's plot the above words use for both New York times and Reuters Health tweets over year of tweets.

```{r}
words_by_time %>% 
  inner_join(top_slopes, by = c("word", "newsgroup")) %>% 
  filter(newsgroup == "nytimeshealth") %>% 
  ggplot(aes(time_floor, count/time_total, color = word))+
  geom_line(size = 1.25)+
  scale_x_date(date_labels = "%b-%Y")+
  labs(x = NULL, y = "Words of frequency")
```


New York time health tweeted a lot about the Ebola which was an outbeak during the last of 2013.

```{r}
words_by_time %>% 
  inner_join(top_slopes, by = c("word", "newsgroup")) %>% 
  filter(newsgroup == "reuters_heatlth") %>% 
  ggplot(aes(time_floor, count/time_total, color = word))+
  geom_line(size = 1.15)+
  scale_x_date(date_labels = "%b-%Y")+
  labs(x = NULL, y = "Words of frequency")
```

Reuters Health also tweeted alot about the Ebola for long period of months as compared to the New York time health tweets, as we see from the graph Reuters tweeted more about "Ebola".

